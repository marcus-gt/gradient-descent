{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE / INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach/Algorithm \n",
    "1. Take the gradient of the loss-function \n",
    "2. Pick random starting values for the parameters that are to be estimated\n",
    "3. Caculate the gradient with the current values\n",
    "4. Calculate the value for the next step: old_value - learning_rate*gradient(current_values)\n",
    "5. Back to step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating some dummy-data\n",
    "For testing/verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f621a50>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# True values of the parameters - what I'm trying to estimate later\n",
    "c_true = 1.5\n",
    "mu_true = 0.5\n",
    "\n",
    "# Genrating some input and outout data, last term adds \n",
    "X = np.random.rand(100,1)\n",
    "y = c_true*(np.exp(-((X-mu_true)**2)/2)) + np.random.rand(100,1)/10\n",
    "plt.plot(X, y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "Picking a single value from the data for each step in the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "### 1) HELPER FUNCTIONS\n",
    "###############################################################################################\n",
    "\n",
    "# 1.1) Function giving av value/vektor of the estimated y's given the current parameters SECTION 3 IN THE TEST\n",
    "def f(c,mu,x_vector):\n",
    "    return c*(np.exp(-((x_vector-mu)**2)/2))\n",
    "\n",
    "\n",
    "# 1.2 Gradient functions (SECTION 5 IN THE TEST)\n",
    "# 1.2.1) Function returning the gradient of the loss-function with respect to the parameter mu\n",
    "def gradient_mu(c,mu,x_vector,y_vector):\n",
    "    N = len(x_vector)\n",
    "    return 2*(f(c,mu,x_vector) - y_vector) * c *(np.exp(-((x_vector-mu)**2)/2))*(x_vector-mu)/2\n",
    "\n",
    "# 1.2.2) Function returning the gradient of the loss-function with respect to the parameter c\n",
    "def gradient_c(c,mu,x_vector,y_vector):\n",
    "    N = len(x_vector)\n",
    "    return 2*(f(c,mu,x_vector) - y_vector) * np.exp(-((x_vector-mu)**2)/2) #/ N\n",
    "\n",
    "###############################################################################################\n",
    "### 2) STOCHASTIUC GRADIENT DESCENT FUNCTIION\n",
    "###############################################################################################\n",
    "\n",
    "def stochastic_gradient_descent(x_vector, y_vector, learning_rate = 0.01, max_num_steps = 1000):    \n",
    "    # Pick random starting values for the parameters c and mu\n",
    "    c = np.random.uniform(0,1,1)\n",
    "    mu = np.random.uniform(0,1,1)\n",
    "    \n",
    "    steps = 0\n",
    "    while steps < max_num_steps:\n",
    "        \n",
    "        # Stochastic element - picking single values for x,y for stochastic gradient descent\n",
    "        permutation = np.random.permutation(len(x_vector))\n",
    "        x = x_vector[permutation][0]\n",
    "        y = y_vector[permutation][0]\n",
    "        \n",
    "        # Calculating new parameters for next step\n",
    "        c = c - learning_rate*gradient_c(c,mu,x,y)\n",
    "        mu = mu - learning_rate*gradient_mu(c,mu,x,y)\n",
    "        \n",
    "        # Updating steps (adding 1)\n",
    "        steps += 1\n",
    "    \n",
    "    return (c,mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real c-value: 1.5 \n",
      "Real c-value: 0.5 \n",
      "Estimated c-value: 1.5573727560314912 \n",
      "Estimated c-value: 0.5080146772330274\n"
     ]
    }
   ],
   "source": [
    "SGD = stochastic_gradient_descent(X,y)\n",
    "print(f\"Real c-value: {c_true} \\nReal c-value: {mu_true} \\nEstimated c-value: {float(SGD[0])} \\nEstimated c-value: {float(SGD[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch (Stochastic) Gradient Descent\n",
    "Selecting a random batch of the data for each step-calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "### 1) HELPER FUNCTIONS\n",
    "###############################################################################################\n",
    "\n",
    "# 1.1) Function giving av value/vector of the estimated y's given the current parameters (same as above)\n",
    "def f(c,mu,x_vector):\n",
    "    return c*(np.exp(-((x_vector-mu)**2)/2))\n",
    "\n",
    "# 1.2 Gradient functions. These differs from the ones above in that they calculate the average gradients for all the observations in the vector (given by N)\n",
    "# 1.2.1) Function returning the gradient of the loss-function with respect to the parameter mu\n",
    "def gradient_c(c, mu, x_vector, y_vector):     \n",
    "    N = len(y_vector)\n",
    "    gradient = 0\n",
    "    for x, y in zip(x_vector, y_vector):\n",
    "        gradient += 2*(f(c, mu, x) - y) * np.exp(-((x-mu)**2)/2)\n",
    "    return gradient/N\n",
    "\n",
    "# 1.2.1) Function returning the gradient of the loss-function with respect to the parameter mu\n",
    "def gradient_mu(c, mu, x_vector, y_vector):         \n",
    "    N = len(y_vector)\n",
    "    gradient = 0\n",
    "    for x,y in zip(x_vector, y_vector):\n",
    "        gradient += 2*(f(c, mu, x) - y) * c *(np.exp(-((x-mu)**2)/2))*(x-mu)/2\n",
    "    return gradient/N\n",
    "\n",
    "###############################################################################################\n",
    "### 2) MINI-BATCH (STOCHASTIC) GRADIENT DESCENT FUNCTIION\n",
    "###############################################################################################\n",
    "\n",
    "def mini_batch_gradient_descent(x_vector, y_vector, learning_rate = 0.01, runs = 1000, batch_size=100):  \n",
    "   # Pick random starting values for the parameters c and mu\n",
    "    c = np.random.uniform(0,1,1)\n",
    "    mu = np.random.uniform(0,1,1)\n",
    "    \n",
    "    # Stochastic element - picking batch of values from the data\n",
    "    permutation = np.random.permutation(len(x_vector))\n",
    "    x = x_vector[permutation][0:batch_size]\n",
    "    y = y_vector[permutation][0:batch_size]    \n",
    "    \n",
    "     # Calculating new parameters\n",
    "    for i in range(runs):\n",
    "        c -= learning_rate*gradient_c(c,mu,x,y)\n",
    "        mu -= learning_rate*gradient_mu(c,mu,x,y)\n",
    "    \n",
    "    return (c,mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real c-value: 1.5 \n",
      "Real c-value: 0.5 \n",
      "Estimated c-value: 1.556134927271715 \n",
      "Estimated c-value: 0.558831524759066\n"
     ]
    }
   ],
   "source": [
    "MBGD = mini_batch_gradient_descent(X,y)\n",
    "print(f\"Real c-value: {c_true} \\nReal c-value: {mu_true} \\nEstimated c-value: {float(MBGD[0])} \\nEstimated c-value: {float(MBGD[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "Matrix with values for the first dirivatives -  Section 4 answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I used a somewhat different approach I never calculated the jacobian explicitly. Here I present a function returning the jacobian as asked in section 5\n",
    "def jacobian_matrix(x_vector, c, mu):\n",
    "    return np.hstack([\n",
    "        c*(np.exp(-((x_vector-mu)**2)/2))*(x_vector-mu)/2    # derivitve of f with respect to c (param 1)\n",
    "        , np.exp(-((x_vector-mu)**2)/2)                      # derivitve of f with respect to mu (param 2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0227246 ,  0.9983823 ],\n",
       "       [ 0.0384886 ,  0.99533815],\n",
       "       [ 0.2176987 ,  0.7876168 ],\n",
       "       [-0.00445113,  0.99993808],\n",
       "       [-0.00169103,  0.99999106],\n",
       "       [ 0.2183674 ,  0.78538734],\n",
       "       [ 0.17889197,  0.87844821],\n",
       "       [ 0.11022774,  0.95960537],\n",
       "       [ 0.05605917,  0.99003053],\n",
       "       [ 0.17220038,  0.88947334],\n",
       "       [ 0.01555535,  0.99924299],\n",
       "       [-0.00285838,  0.99997447],\n",
       "       [ 0.22443115,  0.76320234],\n",
       "       [ 0.06593409,  0.98612687],\n",
       "       [ 0.13823522,  0.93381134],\n",
       "       [ 0.2046192 ,  0.82518203],\n",
       "       [ 0.2227382 ,  0.76979498],\n",
       "       [-0.05703624,  0.9896744 ],\n",
       "       [ 0.23026946,  0.73719966],\n",
       "       [-0.06226425,  0.98765699],\n",
       "       [ 0.21136119,  0.8070944 ],\n",
       "       [ 0.2244109 ,  0.76328331],\n",
       "       [ 0.01339501,  0.99943882],\n",
       "       [ 0.02279585,  0.99837212],\n",
       "       [ 0.23193885,  0.72850423],\n",
       "       [ 0.14981294,  0.92056941],\n",
       "       [ 0.21431208,  0.79837072],\n",
       "       [ 0.14860874,  0.92202736],\n",
       "       [ 0.00264849,  0.99997808],\n",
       "       [ 0.18162432,  0.87366986],\n",
       "       [ 0.12442196,  0.94754369],\n",
       "       [ 0.13146277,  0.94080673],\n",
       "       [ 0.2304303 ,  0.73639198],\n",
       "       [ 0.21121727,  0.80750607],\n",
       "       [-0.05844389,  0.98914981],\n",
       "       [ 0.22404769,  0.76472653],\n",
       "       [ 0.19881292,  0.83908991],\n",
       "       [ 0.21957781,  0.78125352],\n",
       "       [-0.01095024,  0.99962508],\n",
       "       [ 0.19760008,  0.84183007],\n",
       "       [ 0.18590692,  0.86582504],\n",
       "       [ 0.06952224,  0.98453846],\n",
       "       [ 0.17207891,  0.88966513],\n",
       "       [ 0.18358892,  0.87012689],\n",
       "       [ 0.04240484,  0.9943326 ],\n",
       "       [ 0.14980163,  0.92058319],\n",
       "       [-0.04867072,  0.99251344],\n",
       "       [ 0.13235204,  0.93991778],\n",
       "       [ 0.22016537,  0.77919884],\n",
       "       [-0.03795511,  0.99546737],\n",
       "       [ 0.20938113,  0.81265359],\n",
       "       [ 0.20357809,  0.82777864],\n",
       "       [ 0.16997897,  0.89293646],\n",
       "       [ 0.0758679 ,  0.98150152],\n",
       "       [ 0.21505107,  0.79609607],\n",
       "       [ 0.07483046,  0.98201818],\n",
       "       [-0.06471188,  0.98664701],\n",
       "       [ 0.20815099,  0.81599879],\n",
       "       [ 0.19126567,  0.85533589],\n",
       "       [ 0.09487126,  0.97058384],\n",
       "       [ 0.15876302,  0.90909219],\n",
       "       [ 0.07754633,  0.98064874],\n",
       "       [ 0.15998323,  0.90743511],\n",
       "       [ 0.21660167,  0.7911953 ],\n",
       "       [ 0.19283815,  0.85210215],\n",
       "       [-0.0088293 ,  0.9997563 ],\n",
       "       [ 0.20619915,  0.82114787],\n",
       "       [-0.02887551,  0.99738413],\n",
       "       [ 0.07701098,  0.98092302],\n",
       "       [ 0.22307049,  0.76852816],\n",
       "       [ 0.21326674,  0.80152483],\n",
       "       [-0.01796126,  0.99899032],\n",
       "       [-0.01712032,  0.99908278],\n",
       "       [ 0.12079759,  0.9508106 ],\n",
       "       [ 0.11870718,  0.95263508],\n",
       "       [ 0.21502694,  0.79617097],\n",
       "       [ 0.22537482,  0.75936876],\n",
       "       [ 0.11190069,  0.95828355],\n",
       "       [ 0.06407619,  0.98691336],\n",
       "       [ 0.10523705,  0.96339808],\n",
       "       [ 0.20679564,  0.81959422],\n",
       "       [ 0.06260584,  0.98751856],\n",
       "       [-0.07225136,  0.98326833],\n",
       "       [ 0.21146746,  0.80678964],\n",
       "       [ 0.09618955,  0.96972018],\n",
       "       [ 0.16890306,  0.89458065],\n",
       "       [-0.05603682,  0.9900386 ],\n",
       "       [ 0.20982005,  0.81144031],\n",
       "       [ 0.0100324 ,  0.99968532],\n",
       "       [ 0.17340642,  0.88755347],\n",
       "       [-0.0226422 ,  0.99839404],\n",
       "       [ 0.212572  ,  0.80358163],\n",
       "       [ 0.22750428,  0.75024094],\n",
       "       [-0.01426791,  0.99936323],\n",
       "       [ 0.08307092,  0.97769234],\n",
       "       [ 0.22597058,  0.75688469],\n",
       "       [ 0.16048678,  0.90674448],\n",
       "       [-0.06541692,  0.98634825],\n",
       "       [ 0.21347468,  0.80090318],\n",
       "       [ 0.05143855,  0.99162651]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With example estimates for the parameters:\n",
    "J_c = 0.8\n",
    "J_mu = 0.2\n",
    "\n",
    "jacobian_matrix(X, J_c, J_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
